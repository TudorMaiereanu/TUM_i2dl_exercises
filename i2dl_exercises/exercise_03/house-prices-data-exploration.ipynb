{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: House Prices Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will explore datasets in which the features are not images, but CSV files with different features, either numeric or categorical. More specifically, we will look at the housing dataset, famous for being used as an introduction to regression problems, which are problems in which we try to predict a continous variable (eg: the price of a house), rather than a discrete one (eg: the label of an image). \n",
    "\n",
    "The challenges of working with such a dataset are different from the ones we encounter when working with image data. Because each feature is basically the answer to one question (eg: 'Where is the house?', 'Does it have a pool?', 'What surface does the house have?' etc), some of the features might be missing or might have invalid values.\n",
    "\n",
    "This notebook can be seen as a starting point to analyize the house prices data. We will load the data and show of some useful pandas functions for visualizing and understanding the data, as well as some transformations useful if we want to later train a model for predicting house prices. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://i.pinimg.com/originals/fa/6a/74/fa6a7435b27da8fc82705f9552806235.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.data.csv_dataset import CSVDataset\n",
    "from exercise_code.data.csv_dataset import FeatureSelectorAndNormalizationTransform\n",
    "from exercise_code.data.dataloader import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us get started by downloading our data. In `exercise_code/data/csv_dataset.py` you can find a class CSVDataset, which is already implemented for you, but you will understand better by following this notebook.\n",
    "\n",
    "Similar to the last exercise, this class automatically downloads the raw data for you. To do so, simply initialize the class as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_url = 'https://cdn3.vision.in.tum.de/~dl4cv/housing_train.zip'\n",
    "i2dl_exercises_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "root_path = os.path.join(i2dl_exercises_path, \"datasets\", 'housing')\n",
    "target_column = 'SalePrice'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CSVDataset(target_column=target_column, root=root_path, download_url=download_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be able to see a file called `housing_train.csv` in `i2dl_exercises/data/housing`.\n",
    "\n",
    "If you wish to redownload the data, simply call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CSVDataset(target_column=target_column, root=root_path, download_url=download_url, force_download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now have a look at the dataset. Before doing so, however, we will manually load the data here, outside the `CSVDataset` class. We can simply load it by calling the Pandas function `read_csv` with the correct path of our dataset.\n",
    "\n",
    "The entire dataset is contained in `i2dl_exercises/data/housing/housing_train.csv` which you have downloaded before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2dl_exercises_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "housing_file_path = os.path.join(i2dl_exercises_path, \"datasets\", \"housing\", \"housing_train.csv\")\n",
    "df = pd.read_csv(housing_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing with data exploration, let's just see a few rows of the newly created `pd.DataFrame`, so that we can see what are the features of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a few rows from the dataset.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Statistics about the numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the describe function we can get an overview about numerical ranges of the features.\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Statistics about the target column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The housing dataset is used for trying to predict the price of a house by looking at several different characteristics of it. Thus, our target column will be the one refering to the sale price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the names of all columns so that we can find the exact name of the target column.\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also look at the same statistics for the target variable.\n",
    "target_column = 'SalePrice'\n",
    "df[target_column].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To better understand the price distribution, we can also look at a histogram of the prices. \n",
    "sns.distplot(df[target_column], kde=False)\n",
    "plt.title(\"Distribution of Sale Price\")\n",
    "plt.ylabel(\"Number of Occurences\")\n",
    "plt.xlabel(\"Sale Price\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Looking at the relationship between a feature and the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can look at the relationship between different features and the target variable.\n",
    "# For example, we can see how (and if) the pool area, overall quality, living area \n",
    "# or the year the house was built influence the price.\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 12))\n",
    "\n",
    "df.plot.scatter('PoolArea', target_column, ax=axes[0][0])\n",
    "df.plot.scatter('OverallQual', target_column, ax=axes[0][1])\n",
    "df.plot.scatter('GrLivArea', target_column, ax=axes[1][0])\n",
    "df.plot.scatter('YearBuilt', target_column, ax=axes[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Plotting correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also plot the correlation between all pairs of features.\n",
    "# Looking at the lightly-coloured cells in the plot, we can discover \n",
    "# features that are correlated. \n",
    "#\n",
    "# For example, 'GarageYrBlt' and 'YearBuilt' have high correlation.\n",
    "# Also, 'GarageCars' and 'GarageArea' are highly correlated (what a surprise!)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "\n",
    "corrmat = df.corr()\n",
    "sns.heatmap(corrmat, vmax=.8, square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saleprice correlation matrix:\n",
    "# We look at the 10 most correlated variables for our target \"Sale Price\".\n",
    "#\n",
    "# This shows that there are certain attributes much more correlated than \n",
    "# the others for SalePrice.\n",
    "\n",
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.set(font_scale=1.25)\n",
    "\n",
    "k = 10 \n",
    "cols = corrmat.nlargest(k, target_column)[target_column].index\n",
    "cm = np.corrcoef(df[cols].values.T)\n",
    "hm = sns.heatmap(cm, \n",
    "                 cbar=True, \n",
    "                 annot=True, \n",
    "                 square=True, \n",
    "                 fmt='.2f', \n",
    "                 annot_kws={'size': 10}, \n",
    "                 yticklabels=cols.values, \n",
    "                 xticklabels=cols.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Pairwise scatterplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we explore the scatter plot of selected attributes.\n",
    "cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\n",
    "sns.set()\n",
    "sns.pairplot(df[cols], height = 2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Cleaning and preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we have a better understanding of the data, we can start preparing it for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Dealing with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values can be missing from a dataset for different reasons and the missing values can be random or not. Even if dropping the rows with missing values seems to be a tempting solution, some algorithms might not support that and you may see an error message like this:\n",
    "\n",
    "<pre><code> ValueError: Input contains NaN, infinity or a value too large for dtype('float64'). </code></pre>\n",
    "\n",
    "Or, if they support it, their performance might be affected, because we might unawarely introduce a bias in the model. In some situations, we might not even have access to too much data, as data collection is an expensive process, so it is too valuable too just throw away an entire row for a single missing value. Instead, we will focus on filling out the missing values.\n",
    "\n",
    "For further explanations regarding missing values and an interactive tutorial, you can check out [this tutorial](https://www.craft.ai/blog/missing-values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's see if there are any missing values in the dataset.\n",
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see how many null entries are for each column.\n",
    "null_columns = df.columns[df.isnull().any()]\n",
    "df[null_columns].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If one column has much more missing values than present ones, \n",
    "# then it is reasonable to completely remove that column.\n",
    "max_number_of_nans = 0.5 * len(df)\n",
    "df = df.loc[:, (df.isnull().sum(axis=0) <= max_number_of_nans)]\n",
    "\n",
    "\n",
    "# We can fill in the missing numerical values using the mean\n",
    "# for each column:\n",
    "df = df.fillna(df.mean())\n",
    "\n",
    "# Let's check if there are still missing values!\n",
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are still categorical variables with missing values.\n",
    "# One simple way to fill them is to choose the value with the highest\n",
    "# frequency on each column. \n",
    "null_columns = df.columns[df.isnull().any()]\n",
    "for column in null_columns:\n",
    "    most_frequent_value = df[column].value_counts().nlargest(n=1).index[0]\n",
    "    df[column].fillna(most_frequent_value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still any null values? No, we're done!\n",
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we only showed how to fill in the missing values using the mean for numeric fields and the mode for categorical variables. A more sophisticated metod for filling in missing values is using a machine learning algorithm (such as decision trees or K-nearest-neighbours) for predicting them, using the columns with no missing values as training data, but this will not be covered in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Dealing with categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So until now we made sure there are no missing values. But we are still left with some categorical data, such as the 'LotShape' feature, which we cannot just feed into an algorithm. We need to transform these values to numerical values first. We will use a [one-hot encoding](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/) for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out possible values of a categorical column.\n",
    "np.unique(df['LotShape'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Pandas, we can convert categorical variables to one-hot encodings simply by using the `get_dummies` function. \n",
    "\n",
    "\n",
    "For example, for the 'LotShape' column we had the values 'IR1', 'IR2', 'IR3' and 'Reg', then after the one-hot encoding process we will have 4 columns instead, 'LotShape_IR1', 'LotShape_IR2', 'LotShape_IR2' and 'LotShape_Reg' and only one of them will be 1 (depending on the original value), while the rest will be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df)\n",
    "\n",
    "# Key error: 'LotShape' does not exist anymore.\n",
    "# df['LotShape']\n",
    "\n",
    "# 'LotShape_IR1' has only the values 0 and 1.\n",
    "np.unique(df['LotShape_IR1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Normalizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as we did with images, we also need to normalize our data, as machine learning algorithms do not perform well when different features have different scales. If we want to normalize a feature $ x $ in the $[0, 1]$ range, then we can use the following formula:\n",
    "\n",
    "$$ x_\\text{new} = \\frac{x - x_\\text{min}}{x_\\text{max} - x_\\text{min}} $$\n",
    "\n",
    "In Pandas, we can use the [min](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.min.html) and [max](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.max.html) DataFrame methods in order to find the minimum and maximum value per column, for each numerical feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas knows how to broadcast the df.min() dataframe (which has\n",
    "# just one row with the minimum value for each column) such that\n",
    "# we can subtract it from df.\n",
    "df = (df - df.min()) / (df.max() - df.min())\n",
    "\n",
    "# Now all values are between 0 and 1.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Implementing a transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, we have shown the usual csv dataset preprocessing steps on the raw data. The aim, however, is to encapsulate both the data and the transforms in a `CSVDataset` object and to apply the transform not on the whole data at once, but rather on each item and only when required (that is, when `CSVDataset.__getitem__` will be called for that specific row). Even though the housing prices dataset is small enough to fit in memory and apply the tranform at once on the whol dataframe, in other situations we might have to deal with much bigger datasets, so it is good practice to implement transforms that are applied on individual rows of the dataframe, especially as this is also the way in which the standard PyTorch transforms are applied.\n",
    "\n",
    "Because in the next exercise we will use the housing dataset for training a small model, we want to implement a transform that does the following:\n",
    "* Selects a few (2-3) numeric columns from the dataframe\n",
    "* Normalizes these selected features between 0 and 1 \n",
    "* Discards the rest of the features\n",
    "\n",
    "In machine learning, it is always important that any preprocessing step we apply on the train data is also applied on the test and validation data. For normalization, we have seen that we need the minimum and maximum value of each numeric column. For filling in missing numeric values (if any), we need the mean value. These values we should pre-compute and feed them to the transform and for the test dataset we should also use it.\n",
    "\n",
    "We have already implemented the transform for you, you can check it out the `FeatureSelectorAndNormalizationTransform` class in `exercise_code/data/csv_dataset.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-read the data first.\n",
    "df = pd.read_csv(housing_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only 2 features to keep plus the target column.\n",
    "selected_columns = ['OverallQual', 'GrLivArea', target_column]\n",
    "mn, mx, mean = df.min(), df.max(), df.mean()\n",
    "\n",
    "column_stats = {}\n",
    "for column in selected_columns:\n",
    "    crt_col_stats = {'min' : mn[column],\n",
    "                     'max' : mx[column],\n",
    "                     'mean': mean[column]}\n",
    "    column_stats[column] = crt_col_stats    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = FeatureSelectorAndNormalizationTransform(column_stats, target_column)\n",
    "train_dataset = CSVDataset(target_column=target_column, root=root_path, download_url=download_url, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as already shown in the cifar10 notebook, we will be able to use the housing prices dataloader for training a model by directly accessing the batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    # do some machine learning (we'll see in the next weeks how!)\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
