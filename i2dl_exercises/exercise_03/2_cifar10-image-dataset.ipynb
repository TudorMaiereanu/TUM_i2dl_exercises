{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10: Image Dataset\n",
    "\n",
    "Throughout this course, we will teach you all basic skills and supply you with all neccessary tools that you need to implement deep neural networks, which is the main focus of this class. However, you should also be proficient with handling data and know how to prepare it for your specific task. In fact, most of the jobs that involve deep learning in industry are very data related so this is an important skill that you have to pick up.\n",
    "\n",
    "Therefore, we we will take a deep dive into data preparation this week by implementing our own datasets and dataloader. In this notebook, we will focus on the image dataset CIFAR-10. The CIFAR-10 dataset consists of 50000 32x32 colour images in 10 classes, which are *plane*, *car*, *bird*, *cat*, *deer*, *dog*, *frog*, *horse*, *ship*, *truck*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing some libraries that we will need along the way, as well as our code files that we will work on throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from exercise_code.data import (\n",
    "    ImageFolderDataset,\n",
    "    RescaleTransform,\n",
    "    NormalizeTransform,\n",
    "    ComposeTransform,\n",
    "    compute_image_mean_and_std\n",
    ")\n",
    "from exercise_code.tests import (\n",
    "    test_image_folder_dataset,\n",
    "    test_rescale_transform,\n",
    "    test_compute_image_mean_and_std,\n",
    "    save_pickle\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Download\n",
    "Let us get started by downloading our data. In `exercise_code/data/image_folder_dataset.py` you can find a class `ImageFolderDataset`, which you will have to complete throughout this notebook.\n",
    "\n",
    "This class automatically downloads the raw data for you. To do so, simply initialize the class as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_url = \"https://cdn3.vision.in.tum.de/~dl4cv/cifar10.zip\"\n",
    "i2dl_exercises_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "cifar_root = os.path.join(i2dl_exercises_path, \"datasets\", \"cifar10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageFolderDataset(\n",
    "    root=cifar_root, \n",
    "    download_url=download_url\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be able to see the images in `i2dl_exercises/datasets/cifar10`, which should contain one subfolder per class, each containing the respective images labeled `0001.png`, `0002.png`, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the dataset will only be downloaded the first time you initialize a dataset class. If, for some reason, your version of the dataset gets corrupted and you wish to redownload it, simply initialize the class with `force_download=True`, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageFolderDataset(\n",
    "    root=cifar_root,\n",
    "    download_url=download_url,\n",
    "    force_download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training any model you should *always* take a look at some samples of your dataset. This way you can make sure that the data input has worked as intended and you can get a feeling for the dataset. \n",
    "\n",
    "Let's load the CIFAR-10 data and visualize a subset of the images. To do so, we use `PIL.Image.open()` to open an image, and then `numpy.asarray()` to cast the image to an numpy array, which will have shape 32x32x3. We will load 7 images per class in this way, and then we use `matplotlib.pyplot` to visualize the images in a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_as_numpy(image_path):\n",
    "    return np.asarray(Image.open(image_path), dtype=float)\n",
    "\n",
    "classes = [\n",
    "    'plane', 'car', 'bird', 'cat', 'deer',\n",
    "    'dog', 'frog', 'horse', 'ship', 'truck',\n",
    "]\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 7\n",
    "for label, cls in enumerate(sorted(classes)):\n",
    "    for i in range(samples_per_class):\n",
    "        image_path = os.path.join(\n",
    "            cifar_root,\n",
    "            cls,\n",
    "            str(i+1).zfill(4) + \".png\"\n",
    "        )  # e.g. cifar10/plane/0001.png\n",
    "        image = np.asarray(Image.open(image_path))  # open image as numpy array\n",
    "        plt_idx = i * num_classes + label + 1  # calculate plot location in the grid\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        plt.imshow(image.astype('uint8'))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)  # plot class names above columns\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: ImageFolderDataset Implementation\n",
    "\n",
    "Loading images as we have done above is a bit cumbersome. Therefore, we will now write a custom **Dataset** class, which takes care of the loading for us. This is always the first thing you have to implement when starting a new deep learning project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Dataset** class is a wrapper that loads the data from a given filepath and returns a dictionary containing already prepared data, as we have done above. Datasets always need to have the following two methods implemented:\n",
    "- `__len__(self)` is a method that should simply calculate and return the number of images in the dataset. After it is implemented, we can simply call it with `len(dataset)`.\n",
    "- `__getitem(self, index)` should return the image with the given index from your dataset. Implementing this will allow you to access your dataset like a list, i.e. you can then simply call `dataset[9]` to access the 10th image in the dataset.\n",
    "\n",
    "Now it is your turn to implement such a dataset class for CIFAR-10. To do so, open `exercise_code/data/image_folder_dataset.py` and implement the following three methods of `ImageFolderDataset`:\n",
    "- `make_dataset(self, file_path)` should load the prepared data from a given file path into a dictionary.\n",
    "- `__len__(self)` should calculate and return the number of images in your dataset.\n",
    "- `__getitem(self, index)` should return the image with the given index from your dataset.\n",
    "\n",
    "Once you're done, run the cells below to check if your implementation is correct.\n",
    "\n",
    "**Hint:** You may want to reuse parts of the 'Data Visualization' code above in your implementation of `make_dataset()` and `__iter__()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageFolderDataset(\n",
    "    root=cifar_root,\n",
    "    download_url=download_url\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_folder_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `__getitem__()` method, you can now access your dataset conveniently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[0][\"image\"])\n",
    "print(dataset[0][\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And due to the `__len__()` and `__getitem__()` methods, you can even iterate over your dataset now! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 0\n",
    "for sample in dataset:\n",
    "    num_samples += 1\n",
    "print(\"Number of samples:\", num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Preprocessing\n",
    "Before training machine learning models, we often need to preprocess our data. For image datasets, two commonly applied techniques are:\n",
    "1. Normalize all images so that each value is either in [-1, 1] or [0, 1]\n",
    "2. Compute the mean image and substract it from all images in the dataset\n",
    "\n",
    "Using your dataset class, you can now easily add those two preprocessing techniques for CIFAR-10, as we will do in the following by implementing the classes `RescaleTransform` and `NormalizeTransform` in `exercise_code/data/image_folder_dataset.py`.\n",
    "\n",
    "These transform classes are callables, meaning that you will be able to simply use them as follows:\n",
    "\n",
    "```transform = Transform()\n",
    "images_transformed = transform(images)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Rescale Images to [0, 1] using RescaleTransform\n",
    "Let's start by implementing `RescaleTransform`. If you look at the `__init__()` method you will notice it has four arguments:\n",
    "* **range_** is the range you wish to rescale your images to. E.g. if you want to scale your images to [-1, 1], you would use `range=(-1, 1)`. By default, we will scale to [0, 1].\n",
    "* **old_range** is the value range of the data prior to rescaling. For uint8 images, this will always be (0, 255).\n",
    "\n",
    "Your task is now to complete the class by implementing the `__call__()` method that applies the transformation to a given set of images.\n",
    "\n",
    "When you're done, run the following cells to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescale_transform = RescaleTransform()\n",
    "dataset_rescaled = ImageFolderDataset(\n",
    "    root=cifar_root,\n",
    "    download_url=download_url,\n",
    "    transform=rescale_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rescale_transform(dataset_rescaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the first image, you should now see that all values are between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_rescaled[0][\"image\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Images to Standard Gaussian using NormalizeTransform\n",
    "Let us now move on to the `NormalizeTransform` class. The `NormalizeTransform` class normalizes images channel-wise and it's `__init__` method has two arguments:\n",
    "* **mean** is the normalization mean, which we substract from the dataset.\n",
    "* **std** is the normalization standard deviation. By scaling our data with a factor of `1/std` we will normalize the standard deviation accordingly.\n",
    "\n",
    "Have a look at the code in `exercise_code/data/image_folder_dataset.py`.\n",
    "\n",
    "What we would like to do now is to normalize our CIFAR-10 images channel-wise to standard normal. To do so, we need to calculate the per-channel image mean and standard deviation first, which we can then provide to `NormalizeTransform` to normalize our data accordingly.\n",
    "\n",
    "Calculating the per-channel image mean and standard deviation is your task. To do so, implement `compute_image_mean_and_std()` in `exercise_code/data/image_folder_dataset.py` and run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaled_images = []\n",
    "for sample in dataset_rescaled:\n",
    "    rescaled_images.append(sample[\"image\"])\n",
    "rescaled_images = np.array(rescaled_images)\n",
    "cifar_mean, cifar_std = compute_image_mean_and_std(rescaled_images)\n",
    "print(\"Mean:\\t\", cifar_mean, \"\\nStd:\\t\", cifar_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your implementation, run the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_compute_image_mean_and_std(cifar_mean, cifar_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the mean and standard deviation you computed to normalize the data we load, simply by adding the `NormalizeTransform` to the list of transformation our dataset applies in `__getitem__()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescale_transform = RescaleTransform()\n",
    "normalize_transform = NormalizeTransform(\n",
    "    mean=cifar_mean,\n",
    "    std=cifar_std\n",
    ")\n",
    "dataset = ImageFolderDataset(\n",
    "    root=cifar_root,\n",
    "    download_url=download_url,\n",
    "    transform=ComposeTransform([rescale_transform, normalize_transform])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[0][\"image\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save your DataLoaders for Submission\n",
    "Now save your dataset and transforms using the following cell. This will save it to a pickle file `models/cifar_dataset.p`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle(\n",
    "    data_dict={\n",
    "        \"dataset\": dataset_rescaled,\n",
    "        \"cifar_mean\": cifar_mean,\n",
    "        \"cifar_std\": cifar_std,\n",
    "    },\n",
    "    file_name=\"cifar_dataset.p\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "1. Always have a look at your data before you start training any models on it.\n",
    "2. Datasets should be organized in corresponding **Dataset** classes that support `__len__` and `__getitem__` methods, which allows us to call `len(dataset)` and `dataset[index]`.\n",
    "3. Data often needs to be preprocessed; such preprocessing can be implemented in **Transform** classes, which are callables that can be simply applied via `data_transformed = transform(data)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Instructions\n",
    "\n",
    "Now, that you have completed the neccessary parts in the notebook, you can go on and submit your files.\n",
    "\n",
    "1. Go on [our submission page](https://dvl.in.tum.de/teaching/submission/), register for an account and login. We use your matriculation number and send an email with the login details to the mail account associated. When in doubt, login into tum online and check your mails there. You will get an id which we need in the next step.\n",
    "2. Navigate to `exercise_code` directory and run the `create_submission.sh` file to create the zip file of your model. This will create a single `zip` file that you need to upload. Otherwise, you can also zip it manually if you don't want to use the bash script.\n",
    "3. Log into [our submission page](https://dvl.in.tum.de/teaching/submission/) with your account details and upload the `zip` file. Once successfully uploaded, you should be able to see the submitted \"dummy_model.p\" file selectable on the top.\n",
    "4. Click on this file and run the submission script. You will get an email with your score as well as a message if you have surpassed the threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/i2dlsubmission.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Goals\n",
    "\n",
    "- Goal: Implement an ImageFolderDataset with transforms for rescaling and normalizing.\n",
    "- To implement: \n",
    "    1. `exercise_code/data/image_folder_dataset.py`: `ImageFolderDataset` - `make_dataset()`, `__len__()`, `__getitem()__`\n",
    "    2. `exercise_code/data/image_folder_dataset.py`: `RescaleTransform`\n",
    "    3. `exercise_code/data/image_folder_dataset.py`: `compute_image_mean_and_std()`\n",
    "- Test cases:\n",
    "  1. Does `make_dataset()` of `ImageFolderDataset` load paths to images only (and not the actual images)?\n",
    "  2. Does `__len__()` of `ImageFolderDataset` return the correct data type?\n",
    "  3. Does `__len__()` of `ImageFolderDataset` return the correct value?\n",
    "  4. Does `__getitem()__` of `ImageFolderDataset` return the correct data type?\n",
    "  5. Does `__getitem()__` of `ImageFolderDataset` load images as numpy arrays with correct shape?\n",
    "  6. Does `__getitem()__` of `ImageFolderDataset` return the data in the correct order?\n",
    "  7. Do values after rescaling with `RescaleTransform` have the correct minimum?\n",
    "  8. Do values after rescaling with `RescaleTransform` have the correct maximum?\n",
    "  9. Does `compute_image_mean_and_std()` compute the correct mean?\n",
    "  10. Does `compute_image_mean_and_std()` compute the correct std?\n",
    "- Reachable points [0, 100]: 0 if not implemented, 100 if all tests passed, 10 per passed test\n",
    "- Threshold to clear exercise: 80\n",
    "- Submission start: __May 11, 2020 23.59__\n",
    "- Submission deadline : __May 17, 2020 23.59__. \n",
    "- You can make multiple submission uptil the deadline. Your __best submission__ will be considered for bonus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
